{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69842a3a-4abd-406b-9b8b-450c6a26001c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 1: Total Spend Per Customer\n",
    "\n",
    "Calculate total amount spent by each customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f04254-759e-4512-b2fe-51af7996aab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import (col, sum as sparkSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf190e6-c690-4463-926d-ea75fe5ae68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|customer_id|amount|\n+-----------+------+\n|          1|   250|\n|          2|   450|\n|          1|   100|\n|          3|   300|\n|          2|   150|\n+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(customer_id=1, amount=250),\n",
    "    Row(customer_id=2, amount=450),\n",
    "    Row(customer_id=1, amount=100),\n",
    "    Row(customer_id=3, amount=300),\n",
    "    Row(customer_id=2, amount=150)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92980c88-5704-488f-852d-c7994fb5b9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>TotalAmountSpent</th></tr></thead><tbody><tr><td>1</td><td>350</td></tr><tr><td>2</td><td>600</td></tr><tr><td>3</td><td>300</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         350
        ],
        [
         2,
         600
        ],
        [
         3,
         300
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "TotalAmountSpent",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_total_per_customer = df.groupBy(\"customer_id\").agg(sparkSum(col(\"amount\")).alias(\"TotalAmountSpent\"))\n",
    "display(df_total_per_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c39bde9f-84fe-4c46-ad74-60e2b3177510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Basically, I used the aggregate sum function to find total amount spent for each customer. I used import alias sparkSum in order to prevent collision with python's native sum() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a991fd6-0729-4e21-ab74-c7cc3a3c2d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 2: Highest Transaction Per Day\n",
    "\n",
    "Find the highest transaction amount for each day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a6cf19-bc7e-4550-8e4a-1bb8fbbfc346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|      date|amount|\n+----------+------+\n|2023-01-01|   100|\n|2023-01-01|   300|\n|2023-01-02|   150|\n|2023-01-02|   200|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import (col, sum as sparkSum, row_number)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    Row(date='2023-01-01', amount=100),\n",
    "    Row(date='2023-01-01', amount=300),\n",
    "    Row(date='2023-01-02', amount=150),\n",
    "    Row(date='2023-01-02', amount=200)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e2015ba-bf99-4664-8efa-4add80e47f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|      date|amount|\n+----------+------+\n|2023-01-01|   300|\n|2023-01-02|   200|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(df.date).orderBy(df.amount.desc())\n",
    "df_ranked = df.withColumn(\"dailyRank\",row_number().over(windowSpec))\n",
    "df_highest = df_ranked.filter(col(\"dailyRank\") == 1) \\\n",
    "    .select([col('date'), col('amount')])\n",
    "df_highest.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9b0d36d-5738-40c7-9e0a-cd49f5379363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here, since we have to find highest transaction per day, the first step would be to re-arrange amount into descending order for each day. This can be achieved with window function rank_number with partition over the date column. After this, we just select each the rows having rank of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "669749af-af19-45d3-896e-5e67e709a28d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 3: Fill Missing Cities With Default\n",
    "\n",
    "Replace null city values with 'Unknown'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bd46da-f3e1-4e72-bcd2-c4159faf7c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|customer_id|  city|\n+-----------+------+\n|          1|Dallas|\n|          2|  NULL|\n|          3|Austin|\n|          4|  NULL|\n+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(customer_id=1, city='Dallas'),\n",
    "    Row(customer_id=2, city=None),\n",
    "    Row(customer_id=3, city='Austin'),\n",
    "    Row(customer_id=4, city=None)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5140b1-2a47-4dff-aaa6-891a516e2990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n|customer_id|   city|\n+-----------+-------+\n|          1| Dallas|\n|          2|Unknown|\n|          3| Austin|\n|          4|Unknown|\n+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_filled = df.fillna({\"city\":\"Unknown\"})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1c54956-5692-436f-b720-32e5ec3c6591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I used fillna() to replace any null values in city column with \"Unknown\" value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8b35eb0-6928-4a11-9eaf-302e61860da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 4: Compute Running Total by Customer\n",
    "\n",
    "Use a window function to compute cumulative sum of purchases per customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ba5667-c2a6-46cf-a390-0291b70a86be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n|customer_id|      date|amount|\n+-----------+----------+------+\n|          1|2023-01-01|   100|\n|          1|2023-01-02|   200|\n|          1|2023-01-03|   300|\n|          2|2023-01-01|   300|\n|          2|2023-01-02|   400|\n|          2|2023-01-04|   500|\n+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import (lag)\n",
    "\n",
    "data = [\n",
    "    Row(customer_id=1, date='2023-01-01', amount=100),\n",
    "    Row(customer_id=1, date='2023-01-02', amount=200),\n",
    "    Row(customer_id=1, date='2023-01-03', amount=300),\n",
    "    Row(customer_id=2, date='2023-01-01', amount=300),\n",
    "    Row(customer_id=2, date='2023-01-02', amount=400),\n",
    "    Row(customer_id=2, date='2023-01-04', amount=500)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f883497-76d1-4b46-9461-d1824ebbe1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>date</th><th>amount</th><th>previousVal</th></tr></thead><tbody><tr><td>1</td><td>2023-01-01</td><td>100</td><td>0</td></tr><tr><td>1</td><td>2023-01-02</td><td>200</td><td>100</td></tr><tr><td>1</td><td>2023-01-03</td><td>300</td><td>200</td></tr><tr><td>2</td><td>2023-01-01</td><td>300</td><td>0</td></tr><tr><td>2</td><td>2023-01-02</td><td>400</td><td>300</td></tr><tr><td>2</td><td>2023-01-04</td><td>500</td><td>400</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2023-01-01",
         100,
         0
        ],
        [
         1,
         "2023-01-02",
         200,
         100
        ],
        [
         1,
         "2023-01-03",
         300,
         200
        ],
        [
         2,
         "2023-01-01",
         300,
         0
        ],
        [
         2,
         "2023-01-02",
         400,
         300
        ],
        [
         2,
         "2023-01-04",
         500,
         400
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "previousVal",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(col(\"customer_id\")).orderBy(col(\"date\"))\n",
    "df_check_lag = df.withColumn(\"previousVal\",lag(col('amount'),1,0).over(windowSpec))\n",
    "display(df_check_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b6d43e-25e9-404e-94d2-89ef66729fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>date</th><th>amount</th><th>previousVal</th><th>Running Total</th></tr></thead><tbody><tr><td>1</td><td>2023-01-01</td><td>100</td><td>0</td><td>100</td></tr><tr><td>1</td><td>2023-01-02</td><td>200</td><td>100</td><td>300</td></tr><tr><td>1</td><td>2023-01-03</td><td>300</td><td>200</td><td>600</td></tr><tr><td>2</td><td>2023-01-01</td><td>300</td><td>0</td><td>300</td></tr><tr><td>2</td><td>2023-01-02</td><td>400</td><td>300</td><td>700</td></tr><tr><td>2</td><td>2023-01-04</td><td>500</td><td>400</td><td>1200</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2023-01-01",
         100,
         0,
         100
        ],
        [
         1,
         "2023-01-02",
         200,
         100,
         300
        ],
        [
         1,
         "2023-01-03",
         300,
         200,
         600
        ],
        [
         2,
         "2023-01-01",
         300,
         0,
         300
        ],
        [
         2,
         "2023-01-02",
         400,
         300,
         700
        ],
        [
         2,
         "2023-01-04",
         500,
         400,
         1200
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "previousVal",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Running Total",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_running_total = df_check_lag.withColumn(\"Running Total\", col('amount')+col('previousVal'))\n",
    "df_running_total = df_check_lag.withColumn(\"Running Total\", sparkSum(col('amount')) \\\n",
    "    .over(windowSpec.rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
    "display(df_running_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7cf8765-b72c-484b-9d95-2ade8c8c7394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This was a new challenge in terms of window function. My first thought was to use lag function to get previous value and calculate sum on the go, but then i realised this would only give me sum of 2 rows max. Then on searching pyspark docs (https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html), Window function has attribute rowsBetween() where i can specify range from unboundedPreceding (which is a very large negative number, so i presume if goes back to rows before current row context) and currentRow, thus giving the running total. Another similar attribute rangeBetween() was also there, but it seems to work on the column value of the row context and not physical position of the row. So, using the sum over the window specification provided the running total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8476a00-5a2e-437a-9308-dd8b059a22ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 5: Average Sales Per Product\n",
    "\n",
    "Find average amount per product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fd098b-efca-48a7-a1fe-fa4871d9c7d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n|product|amount|\n+-------+------+\n|      A|   100|\n|      B|   200|\n|      A|   300|\n|      B|   400|\n+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import (avg as sparkAvg)\n",
    "\n",
    "data = [\n",
    "    Row(product='A', amount=100),\n",
    "    Row(product='B', amount=200),\n",
    "    Row(product='A', amount=300),\n",
    "    Row(product='B', amount=400)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab51956b-b6c2-488b-a05c-9b5068ac44a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n|product|average_amount|\n+-------+--------------+\n|      A|         200.0|\n|      B|         300.0|\n+-------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "data_avg_product = df.groupBy(col(\"product\")) \\\n",
    "    .agg(sparkAvg(col(\"amount\")).alias(\"average_amount\"))\n",
    "data_avg_product.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e4102fe-dca8-4ad3-9734-e8aa105b26da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since the goal is to find average per product, first we group the data by product name and apply the aggregate function avg() on the amount column. Again, sparkAvg is used as alias to avg to avoid conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86b4d523-d9fc-429c-8119-3eb49db448e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 6: Extract Year From Date\n",
    "\n",
    "Add a column to extract year from given date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d173a7f-ab9a-4919-b11f-6e80ff9e14cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n|customer|transaction_date|\n+--------+----------------+\n|    John|      2022-11-01|\n|   Alice|      2023-01-01|\n+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import (year as sparkYear)\n",
    "\n",
    "data = [\n",
    "    Row(customer='John', transaction_date='2022-11-01'),\n",
    "    Row(customer='Alice', transaction_date='2023-01-01')\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2636b89f-0bbf-4aef-96be-b63a0730ff03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----+\n|customer|transaction_date|Year|\n+--------+----------------+----+\n|    John|      2022-11-01|2022|\n|   Alice|      2023-01-01|2023|\n+--------+----------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df_extract = df.withColumn(\"Year\", sparkYear(col(\"transaction_date\")))\n",
    "df_extract.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea1a8240-6350-4d0b-bacd-1e1a2efd5a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A simple year() function, like in sql, can extract year from the date column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f56c28e2-df4d-441e-81aa-c0af52e6cbad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 7: Join Product and Sales Data\n",
    "\n",
    "Join two DataFrames on product_id to get product names with amounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7240a4e2-a0b9-4a32-bbcc-68122f5cadf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|product_id|product_name|\n+----------+------------+\n|         1|       Phone|\n|         2|      Tablet|\n+----------+------------+\n\n+----------+------+\n|product_id|amount|\n+----------+------+\n|         1|   500|\n|         2|   800|\n|         1|   200|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "products = [\n",
    "    Row(product_id=1, product_name='Phone'),\n",
    "    Row(product_id=2, product_name='Tablet')\n",
    "]\n",
    "sales = [\n",
    "    Row(product_id=1, amount=500),\n",
    "    Row(product_id=2, amount=800),\n",
    "    Row(product_id=1, amount=200)\n",
    "]\n",
    "df_products = spark.createDataFrame(products)\n",
    "df_sales = spark.createDataFrame(sales)\n",
    "df_products.show()\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9a16f4-0771-4653-b8ca-c4bc2bf2adef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>product_name</th><th>amount</th></tr></thead><tbody><tr><td>1</td><td>Phone</td><td>200</td></tr><tr><td>1</td><td>Phone</td><td>500</td></tr><tr><td>2</td><td>Tablet</td><td>800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Phone",
         200
        ],
        [
         1,
         "Phone",
         500
        ],
        [
         2,
         "Tablet",
         800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_joined = df_products.join(df_sales, df_products.product_id == df_sales.product_id, how=\"inner\") \\\n",
    "    .select([df_products['*'], df_sales.amount])\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5cbd915-ea2e-469d-8cc6-d19849ad27bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Explanation:\n",
    "I used the inner join to show product name and sales amount in a single table. Also, I used select function to select all columns of product table and just amount from sales in order to hide duplicate product_id column due to the resulting join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d928839-9cb6-4751-923a-720006416bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 8: Split Tags Into Rows\n",
    "\n",
    "Given a list of comma-separated tags, explode them into individual rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d45ce1-a5f7-4189-961a-6c61d8104d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n| id|        tags|\n+---+------------+\n|  1|   tech,news|\n|  2|sports,music|\n|  3|        food|\n+---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import (split, explode)\n",
    "\n",
    "data = [\n",
    "    Row(id=1, tags='tech,news'),\n",
    "    Row(id=2, tags='sports,music'),\n",
    "    Row(id=3, tags='food')\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d8f561-fefd-4f6e-9c30-fccae253644c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n| id|     split_tags|\n+---+---------------+\n|  1|   [tech, news]|\n|  2|[sports, music]|\n|  3|         [food]|\n+---+---------------+\n\n3\n"
     ]
    }
   ],
   "source": [
    "df_split = df.select([col('id'), split(col('tags'),',').alias(\"split_tags\")])\n",
    "df_split.show()\n",
    "print(df_split.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef42b021-bc30-4763-8300-8cf10ec0338f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n| id|individual_tag|\n+---+--------------+\n|  1|          tech|\n|  1|          news|\n|  2|        sports|\n|  2|         music|\n|  3|          food|\n+---+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_exploded = df_split.withColumn(\"individual_tag\",explode(col(\"split_tags\")))\n",
    "#df_exploded.show()\n",
    "df_final = df_exploded.drop(col(\"split_tags\"))\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "953cdf29-6ba1-4f50-a589-09869add5b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I searched for function to split the string separated by \",\". This created a list of tags in the \"split_tags\" column but the records were still in the same row. So, based on instructions given, I again searched for \"explode\" function which was specifically used to generate new rows in dataframe based on array values in column provided in the argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc05efb-ee84-4062-8ae5-6db878cb8775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 9: Top-N Records Per Group\n",
    "\n",
    "For each category, return top 2 records based on score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ce17b2-b914-4ef2-8b71-fa97a7dfb761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n|category|name|score|\n+--------+----+-----+\n|       A|   x|   80|\n|       A|   y|   90|\n|       A|   z|   70|\n|       B|   p|   60|\n|       B|   q|   85|\n+--------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(category='A', name='x', score=80),\n",
    "    Row(category='A', name='y', score=90),\n",
    "    Row(category='A', name='z', score=70),\n",
    "    Row(category='B', name='p', score=60),\n",
    "    Row(category='B', name='q', score=85)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27bb80bf-3abf-4a0c-af1e-e076242619ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+-----------+\n|category|name|score|rankedScore|\n+--------+----+-----+-----------+\n|       A|   y|   90|          1|\n|       A|   x|   80|          2|\n|       A|   z|   70|          3|\n|       B|   q|   85|          1|\n|       B|   p|   60|          2|\n+--------+----+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(col('category')).orderBy(col('score').desc())\n",
    "df_ranked = df.withColumn(\"rankedScore\", row_number().over(windowSpec))\n",
    "df_ranked.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b6e3a5-7633-467e-a354-cedc33ca2d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n|category|name|score|\n+--------+----+-----+\n|       A|   y|   90|\n|       A|   x|   80|\n|       B|   q|   85|\n|       B|   p|   60|\n+--------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_top2 = df_ranked.filter(col(\"rankedScore\") <= 2).select([\n",
    "    col(\"category\"), col(\"name\"), col(\"score\")\n",
    "])\n",
    "df_top2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ea6f853-298b-4d62-ba4b-82e52c67a47f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This was similar to **Challenge 2** but instead of just getting highest(top 1), we need to get top 2 for each category. So, a ranking function (row_number()) follwed by filter for rank column <= 2 would give the top 2 records for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c34c9d79-5373-4231-b953-14f8c1450504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 10: Null Safe Join\n",
    "\n",
    "Join two datasets where join key might have nulls, handle using null-safe join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d995ec89-2f1c-4618-9f40-753973d78171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n|  id|   name|\n+----+-------+\n|   1|   John|\n|NULL|   Mike|\n|   2|  Alice|\n|NULL|Johnson|\n+----+-------+\n\n+----+------+\n|  id|salary|\n+----+------+\n|   1|  5000|\n|NULL|  3000|\n|NULL|  2000|\n|NULL|  1500|\n+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data1 = [\n",
    "    Row(id=1, name='John'),\n",
    "    Row(id=None, name='Mike'),\n",
    "    Row(id=2, name='Alice'),\n",
    "    Row(id=None, name='Johnson'),\n",
    "]\n",
    "data2 = [\n",
    "    Row(id=1, salary=5000),\n",
    "    Row(id=None, salary=3000),\n",
    "    Row(id=None, salary=2000),\n",
    "    Row(id=None, salary=1500)\n",
    "]\n",
    "df1 = spark.createDataFrame(data1)\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df1.show()\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d626ffbd-3153-4d39-b27b-287a86d78f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+\n|  id|   name|  id|salary|\n+----+-------+----+------+\n|   1|   John|   1|  5000|\n|NULL|   Mike|NULL|  1500|\n|NULL|   Mike|NULL|  2000|\n|NULL|   Mike|NULL|  3000|\n|NULL|Johnson|NULL|  1500|\n|NULL|Johnson|NULL|  2000|\n|NULL|Johnson|NULL|  3000|\n+----+-------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_joined_null_safe = df1.join(df2, df1.id.eqNullSafe(df2.id), how='inner')\n",
    "df_joined_null_safe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a3b7f01-ce36-4226-860d-39a3582169a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I searched and found the null safe join operator in pyspark which is eqNullSafe. Using this the null values are also included in the joined table. TO visualize more clearly, i added 1 more null row in both tables, and as expected, it joins null rows in combination (mxn). Also, it didnot matter which join (left, right, inner) i used, the _null rows_ were the same in count."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Interview_Challenges",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}